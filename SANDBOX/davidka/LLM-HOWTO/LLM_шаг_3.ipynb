{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hypo69/hypotez/blob/master/SANDBOX/davidka/LLM-HOWTO/LLM_%D1%88%D0%B0%D0%B3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch accelerate huggingface_hub  #--quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "6xKeyir3r25U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–î–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —è –≤—ã–±—Ä–∞–ª –º–æ–¥–µ–ª—å *distilbert-base-uncased*"
      ],
      "metadata": {
        "id": "rgj1Qqb0XXVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### –ú–æ–¥–µ–ª—å `\"distilbert-base-uncased\"`\n",
        "\n",
        "‚Äî –æ–±–ª–µ–≥—á—ë–Ω–Ω–∞—è (–¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è) –≤–µ—Ä—Å–∏—è BERT. –ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ —á–∞—Å—Ç—è–º:\n",
        "\n",
        " **1. `distilbert`**  \n",
        "- –≠—Ç–æ **–¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è (—Å–∂–∞—Ç–∞—è) –≤–µ—Ä—Å–∏—è BERT**.  \n",
        "- –û–±—É—á–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–º **knowledge distillation** (–ø–µ—Ä–µ–Ω–æ—Å –∑–Ω–∞–Ω–∏–π –∏–∑ –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ `bert-base-uncased` –≤ –º–µ–Ω—å—à—É—é).  \n",
        "- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç ~95% –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ BERT, –Ω–æ **–≤ 2 —Ä–∞–∑–∞ –±—ã—Å—Ç—Ä–µ–µ** –∏ **–Ω–∞ 40% –º–µ–Ω—å—à–µ** –ø–æ —Ä–∞–∑–º–µ—Ä—É.\n",
        "\n",
        " **2. `base`**  \n",
        "- –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: **\"base\"** (12 —Å–ª–æ—ë–≤, 768 —Å–∫—Ä—ã—Ç—ã—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π, 110 –º–ª–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤).  \n",
        "- –ï—Å—Ç—å —Ç–∞–∫–∂–µ `tiny`, `mini`, `small` –¥–ª—è –µ—â—ë –±–æ–ª–µ–µ –ª—ë–≥–∫–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.\n",
        "\n",
        " **3. `uncased`**  \n",
        "- –ú–æ–¥–µ–ª—å **–Ω–µ —Ä–∞–∑–ª–∏—á–∞–µ—Ç —Ä–µ–≥–∏—Å—Ç—Ä –±—É–∫–≤** (–≤—Å–µ —Ç–µ–∫—Å—Ç—ã –ø—Ä–∏–≤–æ–¥—è—Ç—Å—è –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π).  \n",
        "- –ù–∞–ø—Ä–∏–º–µ—Ä: `\"Hello\"` –∏ `\"hello\"` –±—É–¥—É—Ç —Å—á–∏—Ç–∞—Ç—å—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏.  \n",
        "- –ï—Å–ª–∏ —Ä–µ–≥–∏—Å—Ç—Ä –≤–∞–∂–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π), —Å–ª–µ–¥—É–µ—Ç –∏–∑–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **`cased`**-–≤–µ—Ä—Å–∏–∏.\n",
        "\n",
        "---\n",
        "\n",
        "#### **–ö–æ–≥–¥–∞ –µ—ë –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å?**  \n",
        "- –î–ª—è –∑–∞–¥–∞—á **–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞**, **NER**, **–≤–æ–ø—Ä–æ—Å–æ-–æ—Ç–≤–µ—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º**.  \n",
        "- –ï—Å–ª–∏ –Ω—É–∂–Ω–æ **—ç–∫–æ–Ω–æ–º–∏—Ç—å —Ä–µ—Å—É—Ä—Å—ã** (Colab/–Ω–æ—É—Ç–±—É–∫ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º GPU).  \n",
        "- –î–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ—Ö–æ–¥–æ–º –Ω–∞ –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `bert-large`).\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "#### **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã**  \n",
        "- **`bert-base-uncased`** ‚Äî –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è BERT (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Ç–æ—á–Ω–µ–µ).  \n",
        "- **`distilroberta-base`** ‚Äî –¥–∏—Å—Ç–∏–ª–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è RoBERTa.  \n",
        "- **`google/electra-small`** ‚Äî –ª—ë–≥–∫–∞—è –º–æ–¥–µ–ª—å.  \n",
        "\n",
        "–ï—Å–ª–∏ –Ω—É–∂–Ω–∞ **–º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞**, üëá  \n",
        "- `\"DeepPavlov/rubert-base-cased\"`  \n",
        "- `\"cointegrated/rubert-tiny2\"` (–æ—á–µ–Ω—å –ª—ë–≥–∫–∞—è).  \n"
      ],
      "metadata": {
        "id": "06GAc00wqjA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive –∏ –ø—É—Ç–∏"
      ],
      "metadata": {
        "id": "0D7dYA4ZWGzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZPfFftZ4mM9",
        "outputId": "8d679d9b-15c8-4b59-8a61-cbd740dc46a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# –ü—É—Ç–∏\n",
        "storage:str = '/content/drive/MyDrive/hypo69/llm'\n",
        "train_data_path:str = f'{storage}/train_data'\n",
        "log_path:str = f'{storage}/logs'\n",
        "results_path:str = f'{storage}/results'\n",
        "checkpoints_path:str = f'{storage}/checkpoints'\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
        "json_train_files_list:list | str = glob.glob(os.path.join(train_data_path, '*.json'))\n",
        "json_train_files_list:list = json_train_files_list if isinstance(json_train_files_list, list) else [json_train_files_list]\n",
        "\n"
      ],
      "metadata": {
        "id": "dWB8ERDcYmXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CSV"
      ],
      "metadata": {
        "id": "EWjetUEEQzPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/hypo69/llam_20250516T064847-195.csv')\n",
        "#dataset = Dataset.from_pandas(df)\n",
        "\n",
        "#train_df = pd.read_csv(\"train.csv\")\n",
        "#eval_df = pd.read_csv(\"val.csv\")\n",
        "\n",
        "#train_dataset = Dataset.from_pandas(train_df)\n",
        "#eval_dataset = Dataset.from_pandas(eval_df)"
      ],
      "metadata": {
        "id": "YAKEyvSDr9jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JSON"
      ],
      "metadata": {
        "id": "rvkBRBCpQ6n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "records:list = []\n",
        "for file_path in json_train_files_list:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "\n",
        "            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π\n",
        "            if isinstance(data, list):\n",
        "                records.extend(data)\n",
        "            # –ï—Å–ª–∏ —ç—Ç–æ –æ–¥–∏–Ω–æ—á–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å\n",
        "            elif isinstance(data, dict):\n",
        "                records.append(data)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}')\n",
        "\n",
        "labels_dict: dict = {}\n",
        "try:\n",
        "    with open(f'{storage}/labels.json', 'r', encoding='utf-8') as f:\n",
        "        labels_dict = json.load(f)\n",
        "except Exception as e:\n",
        "    print"
      ],
      "metadata": {
        "id": "UG0hZZ7xQ9w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(records)\n",
        "# –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –≤ —Å—Ç–æ–ª–±—Ü–µ 'text'\n",
        "df_cleaned = df.dropna(subset=['text'])  # –£–¥–∞–ª—è–µ—Ç None/NaN\n",
        "df_cleaned = df_cleaned[df_cleaned['text'].astype(str).str.strip() != \"\"]  # –£–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "print(f\"–ë—ã–ª–æ —Å—Ç—Ä–æ–∫: {len(df)}, —Å—Ç–∞–ª–æ: {len(df_cleaned)}\")\n",
        "print(\"–ü—Ä–∏–º–µ—Ä—ã –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤:\")\n",
        "print(df_cleaned['text'].head())\n"
      ],
      "metadata": {
        "id": "sU9okl2PE4i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset:Dataset = Dataset.from_pandas(df_cleaned)\n",
        "dataset = dataset.train_test_split(test_size=0.2)  # 80% train, 20% eval\n",
        "train_dataset:Dataset = dataset[\"train\"]\n",
        "eval_dataset:Dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "cTMzq3--zS6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è"
      ],
      "metadata": {
        "id": "S3PmIEnzze4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ú–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã (512 —Ç–æ–∫–µ–Ω–æ–≤)\n",
        "\n",
        "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (461 —Ç–æ–∫–µ–Ω –≤–º–µ—Å—Ç–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö 41)\n",
        "\n",
        "–ü—Ä–æ–±–ª–µ–º–∞ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ —Å–æ–∑–¥–∞—Ç—å –±–∞—Ç—á–∏ –∏–∑ —Ä–∞–∑—Ä–æ–∑–Ω–µ–Ω–Ω—ã—Ö –ø–æ –¥–ª–∏–Ω–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π"
      ],
      "metadata": {
        "id": "iaEeoA0_GIpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "–ú–æ–¥—É–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
        "================================================================\n",
        "–ú–æ–¥—É–ª—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–ª–∞—Å—Å TextClassificationPipeline, –∫–æ—Ç–æ—Ä—ã–π –∏–Ω–∫–∞–ø—Å—É–ª–∏—Ä—É–µ—Ç\n",
        "–ª–æ–≥–∏–∫—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏, –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
        "–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:\n",
        "    - transformers\n",
        "    - datasets\n",
        "    - torch\n",
        "    - numpy\n",
        "    - pandas\n",
        "    - huggingface_hub (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ Hub)\n",
        "\"\"\"\n",
        "\n",
        "from typing import Dict, Tuple, Any, Optional\n",
        "from pathlib import Path # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Path –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—É—Ç—è–º–∏\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers.tokenization_utils_base import BatchEncoding\n",
        "from transformers.trainer_utils import EvalPrediction\n",
        "from datasets import DatasetDict, Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "\n",
        "class TextClassificationPipeline:\n",
        "    \"\"\"\n",
        "    –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model_name: str = 'distilbert-base-uncased',\n",
        "                 num_labels: int = 40,\n",
        "                 hub_model_id: Optional[str] = None,\n",
        "                 checkpoints_path: str = './results',\n",
        "                 labels_map_dict: Optional[Dict[str, int]] = None):\n",
        "\n",
        "        \"\"\"\n",
        "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞.\n",
        "\n",
        "        Args:\n",
        "            model_name (str, optional): –ò–º—è –∏–ª–∏ –ø—É—Ç—å –∫ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.\n",
        "            num_labels (int, optional): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.\n",
        "            hub_model_id (Optional[str], optional): ID –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face Hub.\n",
        "            checkpoints_path (str, optional): –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤.\n",
        "                                              –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é './results'.\n",
        "        \"\"\"\n",
        "        self.tokenizer: AutoTokenizer\n",
        "        self.model: AutoModelForSequenceClassification\n",
        "        self.num_labels: int\n",
        "        self.hub_model_id: Optional[str] = hub_model_id\n",
        "        self.checkpoints_path: str = checkpoints_path # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
        "\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                model_name,\n",
        "                num_labels=num_labels,\n",
        "                problem_type='single_label_classification'\n",
        "            )\n",
        "            self.num_labels = num_labels\n",
        "        except OSError as ex:\n",
        "            print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∏–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {model_name}\\nException: {ex}\")\n",
        "            raise\n",
        "        except Exception as ex:\n",
        "            print(f\"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TextClassificationPipeline: {model_name}\\nException: {ex}\")\n",
        "            raise\n",
        "\n",
        "    def tokenize(self,\n",
        "                 input_data: Dict[str, Any],\n",
        "                 max_length: int = 512,\n",
        "                 stride_ratio: float = 0.5) -> BatchEncoding:\n",
        "        \"\"\"\n",
        "        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º (–≤–Ω–∞—Ö–ª–µ—Å—Ç).\n",
        "        # ... (–æ—Å—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ Args –∏ Returns) ...\n",
        "        \"\"\"\n",
        "        tokenized: BatchEncoding\n",
        "\n",
        "        actual_stride = min(max(0, int(max_length * stride_ratio)), max_length -1)\n",
        "        if actual_stride <= 0 and max_length > 0 :\n",
        "            if stride_ratio > 0 :\n",
        "                 print(f\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: stride_ratio > 0, –Ω–æ max_length ({max_length}) —Å–ª–∏—à–∫–æ–º –º–∞–ª. Stride —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ 1.\")\n",
        "                 actual_stride = 1\n",
        "            else:\n",
        "                 actual_stride = 0\n",
        "\n",
        "        print(f\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å max_length={max_length}, stride={actual_stride} (stride_ratio={stride_ratio})\")\n",
        "\n",
        "        tokenized = self.tokenizer(\n",
        "            input_data['text'],\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            stride=actual_stride,\n",
        "            return_overflowing_tokens=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        if 'labels' in input_data and 'overflow_to_sample_mapping' in tokenized:\n",
        "            original_labels = input_data['labels']\n",
        "            new_labels = []\n",
        "            for i in range(len(tokenized['input_ids'])):\n",
        "                original_sample_index = tokenized['overflow_to_sample_mapping'][i]\n",
        "                new_labels.append(original_labels[original_sample_index])\n",
        "            tokenized['labels'] = new_labels\n",
        "        elif 'labels' in input_data:\n",
        "            tokenized['labels'] = input_data['labels']\n",
        "\n",
        "        # ---- –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨ ----\n",
        "        # –£–¥–∞–ª—è–µ–º –ø–æ–ª–µ 'overflow_to_sample_mapping', —Ç–∞–∫ –∫–∞–∫ –æ–Ω–æ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω–æ\n",
        "        # –∏ –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É, –µ—Å–ª–∏ –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω–æ –≤ –º–æ–¥–µ–ª—å.\n",
        "        if 'overflow_to_sample_mapping' in tokenized:\n",
        "            del tokenized['overflow_to_sample_mapping']\n",
        "        # –¢–∞–∫–∂–µ, –µ—Å–ª–∏ –≤–∞—à —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–±–∞–≤–ª—è–µ—Ç 'offset_mapping' –∏ –æ–Ω –Ω–µ –Ω—É–∂–µ–Ω –º–æ–¥–µ–ª–∏,\n",
        "        # –µ–≥–æ —Ç–æ–∂–µ –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –∑–¥–µ—Å—å, —Ö–æ—Ç—è –æ–±—ã—á–Ω–æ –º–æ–¥–µ–ª–∏ –µ–≥–æ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç, –µ—Å–ª–∏ –æ–Ω –Ω–µ –≤ –∏—Ö —Å–∏–≥–Ω–∞—Ç—É—Ä–µ.\n",
        "        # if 'offset_mapping' in tokenized:\n",
        "        #     del tokenized['offset_mapping']\n",
        "        # --------------------------\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def prepare_datasets(self, dataset_dict: DatasetDict) -> Tuple[Dataset, Dataset] | Tuple[None, None]:\n",
        "        \"\"\" –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è) –¥–∞—Ç–∞—Å–µ—Ç–æ–≤. \"\"\"\n",
        "        tokenized_datasets: DatasetDict\n",
        "        error_message: str\n",
        "\n",
        "        for split_name, current_dataset in dataset_dict.items():\n",
        "            if 'labels' not in current_dataset.column_names:\n",
        "                error_message = f\"–î–∞—Ç–∞—Å–µ—Ç '{split_name}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 'labels'. –ö–æ–ª–æ–Ω–∫–∏: {current_dataset.column_names}\"\n",
        "                print(error_message)\n",
        "                raise ValueError(error_message)\n",
        "            if 'text' not in current_dataset.column_names:\n",
        "                error_message = f\"–î–∞—Ç–∞—Å–µ—Ç '{split_name}' –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç 'text'. –ö–æ–ª–æ–Ω–∫–∏: {current_dataset.column_names}\"\n",
        "                print(error_message)\n",
        "                raise ValueError(error_message)\n",
        "\n",
        "        try:\n",
        "            print(f\"–ù–∞—á–∞–ª–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤: {list(dataset_dict.keys())}...\")\n",
        "            tokenized_datasets = dataset_dict.map(\n",
        "                self.tokenize,\n",
        "                batched=True,\n",
        "                remove_columns=['text']\n",
        "            )\n",
        "            print(\"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n",
        "        except Exception as ex:\n",
        "            print(f\"–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {ex}\")\n",
        "            return None, None\n",
        "\n",
        "        if 'train' not in tokenized_datasets or 'test' not in tokenized_datasets:\n",
        "            print(f\"–ö–ª—é—á–∏ 'train' –∏–ª–∏ 'test' –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç. –ö–ª—é—á–∏: {tokenized_datasets.keys()}\")\n",
        "            return None, None\n",
        "\n",
        "        return tokenized_datasets['train'], tokenized_datasets['test']\n",
        "\n",
        "    def train(self, train_dataset: Dataset, eval_dataset: Dataset) -> None:\n",
        "        \"\"\" –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. \"\"\"\n",
        "        training_args: TrainingArguments\n",
        "        trainer: Trainer\n",
        "\n",
        "        if not train_dataset:\n",
        "            print(\"–û—à–∏–±–∫–∞: –û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç.\")\n",
        "            return\n",
        "\n",
        "        per_device_batch_size: int = 8\n",
        "        steps_per_epoch: int = math.ceil(len(train_dataset) / per_device_batch_size)\n",
        "        if steps_per_epoch == 0:\n",
        "            steps_per_epoch = 1\n",
        "        print(f\"–†–∞—Å—Å—á–∏—Ç–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –Ω–∞ —ç–ø–æ—Ö—É: {steps_per_epoch}\")\n",
        "\n",
        "        args_dict = {\n",
        "            'output_dir': self.checkpoints_path,\n",
        "            'per_device_train_batch_size': per_device_batch_size,\n",
        "            'per_device_eval_batch_size': 8,\n",
        "            'num_train_epochs': 3,\n",
        "            'do_eval': True,\n",
        "            'eval_steps': steps_per_epoch,\n",
        "            'logging_steps': steps_per_epoch,\n",
        "            'logging_first_step': True,\n",
        "            'save_steps': steps_per_epoch,\n",
        "            'remove_unused_columns': False,\n",
        "            # 'overwrite_output_dir': True, # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, –¥–ª—è –ø–µ—Ä–µ–∑–ø–∏—Å–∏ output_dir –±–µ–∑ –æ—à–∏–±–æ–∫\n",
        "        }\n",
        "\n",
        "        if self.hub_model_id:\n",
        "            args_dict['push_to_hub'] = True\n",
        "            args_dict['hub_model_id'] = self.hub_model_id\n",
        "            print(f\"–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ Hugging Face Hub –∫–∞–∫: {self.hub_model_id}\")\n",
        "            # –î–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ Hub, –µ—Å–ª–∏ push_to_hub=True:\n",
        "            # args_dict['hub_strategy'] = \"every_save\" # –ó–∞–≥—Ä—É–∂–∞—Ç—å –ø—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
        "            # args_dict['hub_strategy'] = \"epoch\" # –ó–∞–≥—Ä—É–∂–∞—Ç—å –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏ (–ø–æ—Ç—Ä–µ–±—É–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π save_strategy)\n",
        "            # args_dict['hub_strategy'] = \"end\" # –ó–∞–≥—Ä—É–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤ —Å–∞–º–æ–º –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è\n",
        "            # –ï—Å–ª–∏ hub_strategy –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–æ–≤–µ–¥–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ–±—ã—á–Ω–æ \"end\" –∏–ª–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å save_strategy.\n",
        "            # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π, –≥–¥–µ —ç—Ç–∏ –æ–ø—Ü–∏–∏ –º–æ–≥—É—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å, –ø–æ–≤–µ–¥–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ç–æ–ª—å–∫–æ \"–≤ –∫–æ–Ω—Ü–µ\".\n",
        "        else:\n",
        "            print(\"–ú–æ–¥–µ–ª—å –ù–ï –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ Hugging Face Hub (hub_model_id –Ω–µ —É–∫–∞–∑–∞–Ω).\")\n",
        "\n",
        "        training_args = TrainingArguments(**args_dict)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "            compute_metrics=self._compute_metrics\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            print(f\"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è... –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω—è—Ç—å—Å—è –≤: {self.checkpoints_path}\")\n",
        "            trainer.train()\n",
        "\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
        "            # –∏–ª–∏ –º–æ–∂–Ω–æ –ø–æ–ª–æ–∂–∏—Ç—å—Å—è –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –≤ self.checkpoints_path\n",
        "            final_model_path = Path(self.checkpoints_path) / \"final_model_after_training\"\n",
        "            trainer.save_model(str(final_model_path))\n",
        "            print(f\"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ –≤ {final_model_path}\")\n",
        "\n",
        "            # –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ Hub (–µ—Å–ª–∏ push_to_hub –≤ TrainingArguments –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –∏–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ —è–≤–Ω–æ)\n",
        "            if self.hub_model_id and not training_args.push_to_hub:\n",
        "                print(f\"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ Hub –≤—Ä—É—á–Ω—É—é: {self.hub_model_id}\")\n",
        "                try:\n",
        "                    # trainer.push_to_hub() # –æ–±—ã—á–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ, –≤–∫–ª—é—á–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ args\n",
        "                    # –∏–ª–∏ –±–æ–ª–µ–µ —è–≤–Ω–æ:\n",
        "                    self.tokenizer.push_to_hub(self.hub_model_id, commit_message=\"Push tokenizer after training\")\n",
        "                    self.model.push_to_hub(self.hub_model_id, commit_message=\"Push model after training\")\n",
        "                    print(f\"–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω—ã –Ω–∞ {self.hub_model_id}\")\n",
        "                except Exception as e_push:\n",
        "                    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä—É—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ –Ω–∞ Hub: {e_push}\")\n",
        "\n",
        "        except Exception as ex:\n",
        "            print(f'–û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {ex}')\n",
        "            raise\n",
        "\n",
        "    def _compute_metrics(self, eval_pred: EvalPrediction) -> Dict[str, float]:\n",
        "        \"\"\" –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫.\n",
        "        –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:\n",
        "          –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏ –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫. –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ Trainer.\n",
        "        –î–µ–π—Å—Ç–≤–∏—è:\n",
        "          - –ò–∑–≤–ª–µ–∫–∞–µ—Ç predictions (–ª–æ–≥–∏—Ç—ã) –∏ labels (–∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏).\n",
        "          - –ü—Ä–∏–º–µ–Ω—è–µ—Ç np.argmax –∫ –ª–æ–≥–∏—Ç–∞–º –≤–¥–æ–ª—å –æ—Å–∏ –∫–ª–∞—Å—Å–æ–≤ (axis=1) –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (ID —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é).\n",
        "          - –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å (accuracy) –∫–∞–∫ –¥–æ–ª—é —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –∏ –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏.\n",
        "          - –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–æ–π, –Ω–∞–ø—Ä–∏–º–µ—Ä, {'accuracy': 0.95}.\n",
        "        Args:\n",
        "          eval_pred (EvalPrediction): –û–±—ä–µ–∫—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π predictions (–ª–æ–≥–∏—Ç—ã –º–æ–¥–µ–ª–∏, —Ç.–µ. —Å—ã—Ä—ã–µ –≤—ã—Ö–æ–¥—ã –¥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è softmax) –∏ label_ids (–∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏).\n",
        "        Returns:\n",
        "          Dict[str, float]: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞.\n",
        "        \"\"\"\n",
        "        predictions_logits: np.ndarray = eval_pred.predictions\n",
        "        labels: np.ndarray = eval_pred.label_ids\n",
        "        predictions: np.ndarray = np.argmax(predictions_logits, axis=1)\n",
        "        accuracy: float | np.float_ = np.mean(predictions == labels)\n",
        "        return {'accuracy': accuracy}\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vwlS-CgGSLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    HF_MODEL_ID: Optional[str] = None\n",
        "    try:\n",
        "        hf_token = userdata.get('HF_TOKEN')\n",
        "        if not hf_token:\n",
        "            print(\"–¢–æ–∫–µ–Ω HF_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–µ–∫—Ä–µ—Ç–∞—Ö.\")\n",
        "        else:\n",
        "            print(\"–¢–æ–∫–µ–Ω HF –ø–æ–ª—É—á–µ–Ω.\")\n",
        "            login(token=hf_token)\n",
        "            print(\"–£—Å–ø–µ—à–Ω–∞—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–∞ HF Hub.\")\n",
        "            HF_MODEL_ID = \"hypo69/my_model_from_existing_datasets\" # <--- –ò–ú–Ø –ú–û–î–ï–õ–ò\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"–°–µ–∫—Ä–µ—Ç HF_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω.\")\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ HF Hub: {e}\")\n",
        "        HF_MODEL_ID = None\n",
        "\n",
        "\n",
        "    if 'checkpoints_path' not in locals() and 'checkpoints_path' not in globals():\n",
        "        checkpoints_path: str = \"./my_final_checkpoints\"\n",
        "        print(f\"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è 'checkpoints_path' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {checkpoints_path}\")\n",
        "\n",
        "\n",
        "    # -------------------------------- labels_dict -----------------\n",
        "    if labels_dict:\n",
        "        calculated_num_labels_main = len(labels_dict)\n",
        "        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è labels_dict\n",
        "        if not all(isinstance(v, int) for v in labels_dict.values()):\n",
        "            print(\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ labels_dict —è–≤–ª—è—é—Ç—Å—è —Ü–µ–ª—ã–º–∏ —á–∏—Å–ª–∞–º–∏.\")\n",
        "        if not all(isinstance(k, str) for k in labels_dict.keys()):\n",
        "            print(\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ –≤—Å–µ –∫–ª—é—á–∏ –≤ labels_dict —è–≤–ª—è—é—Ç—Å—è —Å—Ç—Ä–æ–∫–∞–º–∏.\")\n",
        "    else:\n",
        "        labels_dict = {} # –ì–∞—Ä–∞–Ω—Ç–∏—è, —á—Ç–æ —ç—Ç–æ –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å, –∞ –Ω–µ None\n",
        "        print(\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: labels_dict –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω. num_labels –±—É–¥–µ—Ç 0.\")\n",
        "\n",
        "    if calculated_num_labels_main == 0:\n",
        "        print(\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ (num_labels) —Ä–∞–≤–Ω–æ 0.\")\n",
        "        # exit()\n",
        "    else:\n",
        "        print(f\"–ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ (num_labels) –¥–ª—è –º–æ–¥–µ–ª–∏: {calculated_num_labels_main}\")\n",
        "\n",
        "\n",
        "    pipeline_instance: Optional[TextClassificationPipeline] = None\n",
        "    dataset_dict_for_pipeline_main: Optional[DatasetDict] = None\n",
        "    prepared_tokenized_datasets: Optional[Tuple[Dataset, Dataset] | Tuple[None, None]] = None\n",
        "    final_tokenized_train_data: Optional[Dataset] = None\n",
        "    final_tokenized_eval_data: Optional[Dataset] = None\n",
        "\n",
        "    try:\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å—É—â–µ—Å—Ç–≤—É—é—Ç –ª–∏ train_dataset –∏ eval_dataset\n",
        "        if ('train_dataset' not in locals() and 'train_dataset' not in globals()) or \\\n",
        "           ('eval_dataset' not in locals() and 'eval_dataset' not in globals()):\n",
        "            raise NameError(\"–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ 'train_dataset' –∏/–∏–ª–∏ 'eval_dataset' –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã. \"\n",
        "                            \"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —è—á–µ–π–∫–∞—Ö.\")\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –æ–Ω–∏ –Ω–µ None (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –æ–±—ä—è–≤–ª–µ–Ω—ã, –Ω–æ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã)\n",
        "        if train_dataset is None or eval_dataset is None:\n",
        "             raise ValueError(\"'train_dataset' –∏–ª–∏ 'eval_dataset' —è–≤–ª—è—é—Ç—Å—è None. \"\n",
        "                              \"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–∑–¥–∞–Ω—ã.\")\n",
        "\n",
        "        print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π train_dataset ({len(train_dataset)} –∑–∞–ø–∏—Å–µ–π) \"\n",
        "              f\"–∏ eval_dataset ({len(eval_dataset)} –∑–∞–ø–∏—Å–µ–π).\")\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–æ–ª–æ–Ω–æ–∫ 'text' –∏ 'labels' –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö\n",
        "        for ds_name, ds_object in [(\"train_dataset\", train_dataset), (\"eval_dataset\", eval_dataset)]:\n",
        "            if ds_object: # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –æ–±—ä–µ–∫—Ç –Ω–µ None\n",
        "                if 'text' not in ds_object.column_names or 'labels' not in ds_object.column_names:\n",
        "                    raise ValueError(f\"{ds_name} –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'text' –∏ 'labels'. \"\n",
        "                                     f\"–¢–µ–∫—É—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏: {ds_object.column_names}\")\n",
        "                # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –≤—Å–µ –º–µ—Ç–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0, num_labels-1]\n",
        "                # –≠—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–∂–Ω–∞, —Ç–∞–∫ –∫–∞–∫ `num_labels` –±–µ—Ä–µ—Ç—Å—è –∏–∑ `labels_dict`\n",
        "                if calculated_num_labels_main > 0: # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –æ–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª-–≤–æ –º–µ—Ç–æ–∫\n",
        "                    all_labels_in_ds = set(ds_object['labels'])\n",
        "                    if all_labels_in_ds and \\\n",
        "                       (max(all_labels_in_ds) >= calculated_num_labels_main or min(all_labels_in_ds) < 0):\n",
        "                        raise ValueError(\n",
        "                            f\"–ú–µ—Ç–∫–∏ –≤ {ds_name} (–¥–∏–∞–ø–∞–∑–æ–Ω: {min(all_labels_in_ds)}-{max(all_labels_in_ds)}) \"\n",
        "                            f\"–≤—ã—Ö–æ–¥—è—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [0, {calculated_num_labels_main-1}], \"\n",
        "                            f\"–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∏–∑ actual_labels_dict. –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏: {sorted(list(all_labels_in_ds))}\"\n",
        "                        )\n",
        "                elif calculated_num_labels_main == 0 and ds_object['labels']: # –ï—Å–ª–∏ actual_labels_dict –ø—É—Å—Ç, –Ω–æ –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –º–µ—Ç–∫–∏\n",
        "                    print(f\"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: actual_labels_dict –ø—É—Å—Ç, –Ω–æ {ds_name} —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–∫–∏. num_labels –±—É–¥–µ—Ç 0, —á—Ç–æ –≤—ã–∑–æ–≤–µ—Ç –æ—à–∏–±–∫—É.\")\n",
        "\n",
        "\n",
        "        if calculated_num_labels_main > 0: # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ num_labels –æ–ø—Ä–µ–¥–µ–ª–µ–Ω\n",
        "            dataset_dict_for_pipeline_main = DatasetDict({\n",
        "                'train': train_dataset,\n",
        "                'test': eval_dataset\n",
        "            })\n",
        "\n",
        "            pipeline_instance = TextClassificationPipeline(\n",
        "                num_labels=calculated_num_labels_main,\n",
        "                hub_model_id=HF_MODEL_ID,\n",
        "                checkpoints_path=checkpoints_path,\n",
        "                labels_map_dict= labels_dict\n",
        "            )\n",
        "        else:\n",
        "            print(\"–û–±—É—á–µ–Ω–∏–µ –Ω–µ –±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–æ, —Ç–∞–∫ –∫–∞–∫ calculated_num_labels_main —Ä–∞–≤–µ–Ω 0 (–ø—Ä–æ–±–ª–µ–º–∞ —Å actual_labels_dict).\")\n",
        "\n",
        "        if pipeline_instance and dataset_dict_for_pipeline_main:\n",
        "            prepared_tokenized_datasets = pipeline_instance.prepare_datasets(dataset_dict_for_pipeline_main)\n",
        "\n",
        "            if prepared_tokenized_datasets and prepared_tokenized_datasets[0] and prepared_tokenized_datasets[1]:\n",
        "                final_tokenized_train_data, final_tokenized_eval_data = prepared_tokenized_datasets\n",
        "                print('–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã:')\n",
        "                print(f'Train: {len(final_tokenized_train_data)} –ø., –ö–æ–ª–æ–Ω–∫–∏: {final_tokenized_train_data.column_names}')\n",
        "                print(f'Eval: {len(final_tokenized_eval_data)} –ø., –ö–æ–ª–æ–Ω–∫–∏: {final_tokenized_eval_data.column_names}')\n",
        "\n",
        "                pipeline_instance.train(final_tokenized_train_data, final_tokenized_eval_data)\n",
        "            else:\n",
        "                print('–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏. –û–±—É—á–µ–Ω–∏–µ –Ω–µ –±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–æ.')\n",
        "        elif not pipeline_instance and calculated_num_labels_main > 0:\n",
        "             print(\"–ü–∞–π–ø–ª–∞–π–Ω –Ω–µ –±—ã–ª –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.\")\n",
        "\n",
        "\n",
        "    except NameError as ne: # –ï—Å–ª–∏ train_dataset –∏–ª–∏ eval_dataset –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç\n",
        "        print(f\"–û—à–∏–±–∫–∞ NameError: {ne}\")\n",
        "        print(\"–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ train_dataset, eval_dataset –∏ labels_dict (–µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∑–¥–µ—Å—å) –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —è—á–µ–π–∫–∞—Ö.\")\n",
        "    except ValueError as ex:\n",
        "        print(f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ (ValueError): {str(ex)}')\n",
        "    except Exception as ex:\n",
        "        import traceback\n",
        "        print(f'–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(ex)}')\n",
        "        print(\"–¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞:\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "KcXNOQZljL0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBcb6SQVW27V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EWjetUEEQzPP"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}