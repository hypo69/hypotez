{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOH/UEFR3lZfrS4OPtWhVu7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hypo69/hypotez/blob/master/SANDBOX/davidka/LLM-HOWTO/LLM_%D1%88%D0%B0%D0%B3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ri2CR6r0Cz9w"
      },
      "outputs": [],
      "source": [
        "%pip install transformers torch sentencepiece\n",
        "# Или tensorflow, или jax, в зависимости от модели\n",
        "# Для некоторых моделей может понадобиться sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Основной способ: Использование AutoModel и AutoTokenizer\n",
        "Это самый гибкий и рекомендуемый способ, так как он автоматически определяет архитектуру модели по её имени на Hub."
      ],
      "metadata": {
        "id": "eoNXW17tDRKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM # или другой AutoModelFor...\n",
        "from huggingface_hub import HfFolder, login\n",
        "import os\n",
        "\n",
        "\n",
        "try:\n",
        "    HF_AUTH_TOKEN = userdata.get('HF_TOKEN')\n",
        "    if HF_AUTH_TOKEN is None:\n",
        "        raise ValueError(\"Секрет 'HF_TOKEN' не найден или пуст. Пожалуйста, добавьте его в Colab Secrets.\")\n",
        "    print(\"Токен Hugging Face успешно получен из Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при получении токена из Colab Secrets: {e}\")\n",
        "    HF_AUTH_TOKEN = None # Устанавливаем в None, чтобы последующий код мог это обработать\n",
        "\n",
        "\n",
        "MODEL_NAME = \"hypo69/my_model_from_existing_datasets\"\n",
        "\n",
        "if HF_AUTH_TOKEN:\n",
        "    try:\n",
        "        # --- Способ 1: Использование login() из huggingface_hub (предпочтительно) ---\n",
        "        # Это сделает токен доступным для всех последующих вызовов from_pretrained\n",
        "        # в текущей сессии, аналогично huggingface-cli login.\n",
        "        login(token=HF_AUTH_TOKEN)\n",
        "        print(f\"Успешная авторизация в Hugging Face Hub\")\n",
        "\n",
        "        # Теперь загружаем модель и токенизатор. Токен будет использован автоматически.\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME) # Укажите правильный AutoModelFor...\n",
        "        print(f\"Модель '{MODEL_NAME}' и токенизатор успешно загружены.\")\n",
        "\n",
        "        # --- Способ 2: Явная передача токена в from_pretrained ---\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_AUTH_TOKEN)\n",
        "        # model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=HF_AUTH_TOKEN)\n",
        "        # print(f\"Модель '{MODEL_NAME}' и токенизатор успешно загружены с явной передачей токена.\")\n",
        "\n",
        "        # --- Способ 3: Установка переменной окружения (менее предпочтительно, чем login()) ---\n",
        "        # os.environ[\"HF_TOKEN\"] = HF_AUTH_TOKEN\n",
        "        # print(\"Переменная окружения HF_TOKEN установлена.\")\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        # model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "        # print(f\"Модель '{MODEL_NAME}' и токенизатор успешно загружены (используя переменную окружения).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке приватной модели или токенизатора: {e}\")\n",
        "        print(\"Убедитесь, что токен действителен и имеет доступ к модели.\")\n",
        "else:\n",
        "    print(\"Токен Hugging Face не был загружен. Невозможно загрузить приватную модель.\")"
      ],
      "metadata": {
        "id": "YYIH3c0VDSFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Типы AutoModelFor...:\n",
        "\n",
        "- AutoModelForCausalLM: Для моделей генерации текста (авторегрессионных, как GPT).\n",
        "\n",
        "- AutoModelForMaskedLM: Для моделей с маскированием (как BERT, RoBERTa).\n",
        "\n",
        "- AutoModelForSequenceClassification: Для задач классификации текста.\n",
        "\n",
        "- AutoModelForQuestionAnswering: Для задач ответа на вопросы.\n",
        "\n",
        "- AutoModelForTokenClassification: Для задач NER, PoS-теггинга.\n",
        "\n",
        "- AutoModelForSeq2SeqLM: Для моделей \"последовательность-в-последовательность\" (перевод, суммаризация, как T5, BART).\n",
        "\n",
        "- AutoModel: Загружает базовую модель без специфической \"головы\" для конкретной задачи."
      ],
      "metadata": {
        "id": "xfemQKAND0up"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6SFTrjjD1f1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}