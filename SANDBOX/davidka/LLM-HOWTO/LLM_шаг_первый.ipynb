{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ZSCiULZF2Fw0AN6PmLEwavDpVgSHm6Ua",
      "authorship_tag": "ABX9TyPPoStYe43aLKieLZg4VeWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hypo69/hypotez/blob/master/SANDBOX/davidka/LLM-HOWTO/LLM_%D1%88%D0%B0%D0%B3_%D0%BF%D0%B5%D1%80%D0%B2%D1%8B%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание сети"
      ],
      "metadata": {
        "id": "3qqClk0cG0q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Важно:** Эта сеть будет **очень простой** (один скрытый слой, ручная реализация без библиотек типа TensorFlow/PyTorch) и **не является LLM (Large Language Model)**.  Эта сеть для иллюстрации базовых принципов нейронных сетей (нейроны, слои, веса, обучение через обратное распространение ошибки).\n",
        "\n",
        "\n",
        "**Объяснение кода:**\n",
        "\n",
        "1.  **Классы (`Input`, `Neuron`, `Layer`, `NeuroNetwork`):** Структура повторяет статью. `Input` хранит вес и ссылку на предыдущий нейрон. `Neuron` содержит значение, список входов, ошибку и дельту для обучения, а также методы для расчета значения. `Layer` группирует нейроны. `NeuroNetwork` объединяет слои, управляет обучением и тестированием.\n",
        "2.  **Инициализация (`__init__`):** В конструкторе `NeuroNetwork` создаются слои нужного размера. Размеры скрытых слоев рассчитываются по формуле из статьи. Веса инициализируются случайно (`random.random()`).\n",
        "3.  **Функции активации (`sigmoid`, `sigmoid_derivative`):** Используется сигмоидальная функция и ее производная, как описано. Добавлена защита от математического переполнения в `sigmoid`.\n",
        "4.  **Прямой проход (`forward_pass`, `get_prediction`):** Данные проходят от входа к выходу. Каждый нейрон (кроме входных) вычисляет свою взвешенную сумму входов (`get_input_sum`) и применяет функцию активации (`calculate_value`). `get_prediction` запускает `forward_pass` и возвращает значения выходных нейронов.\n",
        "5.  **Обратное распространение ошибки (`backpropagate`):** Это ядро обучения.\n",
        "    *   Сначала вычисляется ошибка и \"дельта\" для выходного слоя (насколько сильно выходной нейрон ошибся и как сильно его входная сумма влияет на ошибку).\n",
        "    *   Затем ошибка \"распространяется\" назад по слоям. Ошибка каждого нейрона скрытого слоя зависит от ошибок нейронов следующего слоя (справа) и весов связей между ними. Рассчитывается дельта для каждого скрытого нейрона.\n",
        "    *   Наконец, зная дельты всех нейронов, веса всех связей корректируются (`w = w + learning_rate * delta_нейрона_справа * значение_нейрона_слева`) так, чтобы уменьшить общую ошибку сети.\n",
        "6.  **Обучение (`train`, `train_once`):** `train` управляет процессом обучения в течение заданного числа `epochs`. В каждой эпохе она проходит по всему `dataset`. `train_once` обрабатывает один пример: выполняет прямой проход, затем обратное распространение для коррекции весов. Датасет перемешивается в каждой эпохе для лучшего обучения.\n",
        "7.  **Тестирование (`test`):** Подает на вход сети тестовые данные, выполняет прямой проход и выводит предсказанный результат. Для задач классификации (как \"ИЛИ\") результат часто округляют до 0 или 1.\n",
        "8.  **Сохранение/Загрузка (`save_model`, `load_model`):** Используется стандартная библиотека Python `pickle`. `save_model` сериализует весь объект `NeuroNetwork` (включая все слои, нейроны и их веса) в файл. `load_model` десериализует объект из файла, позволяя восстановить обученную сеть.\n",
        "\n",
        "\n",
        "**Следующие шаги (Куда двигаться дальше):**\n",
        "\n",
        "*   **Поэкспериментировать:** Изменить скорость обучения (`learning_rate`), количество скрытых нейронов, количество эпох, попробовать другую функцию активации (например, ReLU).\n",
        "*   **Другие задачи:** Попробовать обучить сеть на других логических операциях (AND, XOR - XOR сложнее и может потребовать более глубокой сети или больше нейронов).\n",
        "*   **Библиотеки:** Изучить библиотеки `NumPy` (для быстрых операций с матрицами), `TensorFlow` или `PyTorch`. Они предоставляют готовые слои, функции активации, оптимизаторы и используют GPU для ускорения вычислений, что необходимо для настоящих LLM.\n",
        "Эта простая сеть **НЕ** LLM. LLM используют архитектуру Transformer, огромные датасеты и триллионы операций для обучения."
      ],
      "metadata": {
        "id": "Yc-GvV5EzZeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Код*"
      ],
      "metadata": {
        "id": "XHXEUy1HGBji"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RM1ECh96zKj4"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from math import exp, ceil\n",
        "import pickle # Для сохранения и загрузки модели\n",
        "\n",
        "# --- Часть 1: Инициализация ---\n",
        "\n",
        "# Класс для входа нейрона (хранит ссылку на предыдущий нейрон и вес связи)\n",
        "class Input:\n",
        "    def __init__(self, prev_neuron, weight):\n",
        "        self.prev_neuron = prev_neuron # Нейрон из предыдущего слоя\n",
        "        self.weight = weight         # Вес связи\n",
        "\n",
        "# Класс Нейрона\n",
        "class Neuron:\n",
        "    def __init__(self, layer, previous_layer):\n",
        "        self._layer = layer # Слой, к которому принадлежит нейрон\n",
        "        self.value = 0.0    # Текущее значение/активация нейрона (инициализируем 0)\n",
        "        self.inputs = []    # Список входящих связей (объекты Input)\n",
        "        self.error = 0.0    # Ошибка нейрона (для обратного распространения)\n",
        "        self.delta = 0.0    # Дельта ошибки (ошибка * производная активации) - для расчета градиентов\n",
        "        self.last_input_sum = 0.0 # Сумма взвешенных входов (нужна для вычисления производной)\n",
        "\n",
        "        # Создаем входы, если это не входной слой (т.е. есть предыдущий слой)\n",
        "        if previous_layer:\n",
        "            self.inputs = [Input(prev_neuron, random.random()) # Случайный вес от 0.0 до 1.0\n",
        "                           for prev_neuron in previous_layer.neurons]\n",
        "\n",
        "    # Установить значение нейрона (используется для входного слоя)\n",
        "    def set_value(self, val):\n",
        "        self.value = float(val) # Убедимся, что значение - float\n",
        "\n",
        "    # Проверить, есть ли у нейрона входы (т.е. не является ли он нейроном входного слоя)\n",
        "    def is_no_inputs(self):\n",
        "        return not self.inputs\n",
        "\n",
        "    # Рассчитать взвешенную сумму входов\n",
        "    def get_input_sum(self):\n",
        "        # Сумма = Σ (значение_предыдущего_нейрона * вес_связи)\n",
        "        total_sum = sum(inp.prev_neuron.value * inp.weight for inp in self.inputs)\n",
        "        return total_sum\n",
        "\n",
        "    # Рассчитать и обновить значение (активацию) нейрона на основе входов\n",
        "    def calculate_value(self):\n",
        "        # Нейроны входного слоя свое значение получают извне (set_value)\n",
        "        if self.is_no_inputs():\n",
        "            return\n",
        "\n",
        "        # 1. Считаем взвешенную сумму входов\n",
        "        input_sum = self.get_input_sum()\n",
        "        self.last_input_sum = input_sum # Сохраняем для расчета производной при обучении\n",
        "\n",
        "        # 2. Применяем функцию активации (сигмоиду)\n",
        "        self.value = self._layer.network.activate_func(input_sum)\n",
        "\n",
        "# Класс Слоя\n",
        "class Layer:\n",
        "    def __init__(self, layer_size, prev_layer, parent_network):\n",
        "        self.prev_layer = prev_layer       # Ссылка на предыдущий слой (None для входного)\n",
        "        self.network = parent_network    # Ссылка на родительскую сеть\n",
        "        # Создаем нейроны для этого слоя\n",
        "        self.neurons = [Neuron(self, prev_layer) for _ in range(layer_size)]\n",
        "\n",
        "    # Установить входные данные для слоя (используется для входного слоя)\n",
        "    def set_input_data(self, val_list):\n",
        "        if len(val_list) != len(self.neurons):\n",
        "            raise ValueError(\"Размер входных данных не совпадает с количеством нейронов входного слоя\")\n",
        "        for i in range(len(val_list)):\n",
        "            self.neurons[i].set_value(val_list[i])\n",
        "\n",
        "# Класс Нейронной Сети\n",
        "class NeuroNetwork:\n",
        "    def __init__(self, input_l_size, output_l_size, hidden_layers_count=1, learning_rate=0.1): # Уменьшил learning_rate для стабильности\n",
        "        self.selected_layer = None # Временный указатель для постройки слоев\n",
        "        self.l_count = hidden_layers_count + 2 # Общее кол-во слоев (входной + скрытые + выходной)\n",
        "\n",
        "        # Формула расчета размера скрытых слоев (из статьи)\n",
        "        # Добавил max(1, ...) чтобы слой не был нулевого размера\n",
        "        hidden_l_size = max(1, min(input_l_size * 2 - 1, ceil(input_l_size * 2 / 3 + output_l_size)))\n",
        "\n",
        "        # --- Параметры обучения ---\n",
        "        self.learning_rate = learning_rate          # Скорость обучения\n",
        "        self.activate_func = NeuroNetwork.sigmoid   # Функция активации\n",
        "        self.derivate_func = NeuroNetwork.sigmoid_derivative # Производная функции активации\n",
        "\n",
        "        # --- Создание слоев ---\n",
        "        self.layers = []\n",
        "        for i in range(self.l_count):\n",
        "            # Определяем размер текущего слоя\n",
        "            if i == 0: # Входной слой\n",
        "                current_layer_size = input_l_size\n",
        "                prev_layer_ref = None\n",
        "            elif i == self.l_count - 1: # Выходной слой\n",
        "                current_layer_size = output_l_size\n",
        "                prev_layer_ref = self.layers[-1] # Последний добавленный слой\n",
        "            else: # Скрытый слой\n",
        "                current_layer_size = hidden_l_size\n",
        "                prev_layer_ref = self.layers[-1] # Последний добавленный слой\n",
        "\n",
        "            # Создаем слой и добавляем в список\n",
        "            new_layer = Layer(current_layer_size, prev_layer_ref, self)\n",
        "            self.layers.append(new_layer)\n",
        "\n",
        "        print(f\"Сеть создана: Входной={input_l_size}, Скрытый={hidden_l_size}x{hidden_layers_count}, Выходной={output_l_size}\")\n",
        "\n",
        "    # --- Статические методы для функций активации ---\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        # Добавим защиту от переполнения для exp()\n",
        "        if x < -700: return 0.0\n",
        "        if x > 700: return 1.0\n",
        "        try:\n",
        "            return 1 / (1 + exp(-x))\n",
        "        except OverflowError:\n",
        "            return 0.0 if x < 0 else 1.0\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(x):\n",
        "        # Производная сигмоиды: sigmoid(x) * (1 - sigmoid(x))\n",
        "        sig_x = NeuroNetwork.sigmoid(x)\n",
        "        return sig_x * (1 - sig_x)\n",
        "\n",
        "    # --- Часть 2: Обучение и Тестирование ---\n",
        "\n",
        "    # Установка входных данных для всей сети\n",
        "    def set_input_data(self, val_list):\n",
        "        self.layers[0].set_input_data(val_list) # Устанавливаем данные на первый (входной) слой\n",
        "\n",
        "    # Прямой проход (Forward Pass) - вычисление результата сети\n",
        "    def forward_pass(self):\n",
        "        # Проходим по всем слоям, начиная со второго (первого скрытого)\n",
        "        for i in range(1, self.l_count):\n",
        "            layer = self.layers[i]\n",
        "            # Каждый нейрон слоя вычисляет свое значение на основе предыдущего слоя\n",
        "            for neuron in layer.neurons:\n",
        "                neuron.calculate_value()\n",
        "\n",
        "    # Получение предсказания (результата) сети\n",
        "    def get_prediction(self):\n",
        "        # Сначала выполняем прямой проход, чтобы обновить все значения\n",
        "        self.forward_pass()\n",
        "        # Собираем значения с нейронов выходного слоя\n",
        "        output_layer = self.layers[-1]\n",
        "        out_data = [neuron.value for neuron in output_layer.neurons]\n",
        "        return out_data\n",
        "\n",
        "    # Обратное распространение ошибки (Backpropagation) и обновление весов для одного примера\n",
        "    def backpropagate(self, expected_output):\n",
        "        output_layer = self.layers[-1]\n",
        "\n",
        "        # Убедимся, что размер ожидаемого выхода совпадает с выходным слоем\n",
        "        if len(expected_output) != len(output_layer.neurons):\n",
        "             raise ValueError(\"Размер ожидаемого выхода не совпадает с размером выходного слоя\")\n",
        "\n",
        "        # 1. Вычисление ошибки и дельты для выходного слоя\n",
        "        for i, neuron in enumerate(output_layer.neurons):\n",
        "            error = expected_output[i] - neuron.value\n",
        "            neuron.error = error\n",
        "            # Дельта = Ошибка * Производная(взвешенная_сумма_входов_нейрона)\n",
        "            neuron.delta = error * self.derivate_func(neuron.last_input_sum)\n",
        "\n",
        "        # 2. Распространение ошибки на скрытые слои (от предпоследнего к первому скрытому)\n",
        "        for i in range(self.l_count - 2, 0, -1): # Идем от слоя l-2 до слоя 1\n",
        "            current_layer = self.layers[i]\n",
        "            next_layer = self.layers[i+1] # Слой справа (ближе к выходу)\n",
        "\n",
        "            for j, neuron in enumerate(current_layer.neurons):\n",
        "                # Ошибка нейрона = Сумма (дельта_нейрона_справа * вес_связи_между_ними)\n",
        "                error_sum = sum(next_neuron.delta * next_neuron.inputs[j].weight\n",
        "                                for next_neuron in next_layer.neurons)\n",
        "                neuron.error = error_sum\n",
        "                # Дельта = Ошибка * Производная(взвешенная_сумма_входов_нейрона)\n",
        "                neuron.delta = error_sum * self.derivate_func(neuron.last_input_sum)\n",
        "\n",
        "        # 3. Обновление весов во всех слоях (начиная с первого скрытого)\n",
        "        for i in range(1, self.l_count): # Идем от слоя 1 до слоя l-1\n",
        "            current_layer = self.layers[i]\n",
        "            for neuron in current_layer.neurons:\n",
        "                for input_connection in neuron.inputs:\n",
        "                    # Обновление веса: w = w + learning_rate * дельта_нейрона * значение_нейрона_слева\n",
        "                    gradient = neuron.delta * input_connection.prev_neuron.value\n",
        "                    input_connection.weight += self.learning_rate * gradient\n",
        "\n",
        "    # Обучение на одном примере данных (прямой проход + обратное распространение)\n",
        "    def train_once(self, input_data, expected_output):\n",
        "        # 1. Устанавливаем входные данные\n",
        "        self.set_input_data(input_data)\n",
        "        # 2. Выполняем прямой проход (чтобы получить текущий выход и last_input_sum)\n",
        "        self.forward_pass()\n",
        "        # 3. Выполняем обратное распространение ошибки и обновляем веса\n",
        "        self.backpropagate(expected_output)\n",
        "        # Можно вернуть ошибку для мониторинга (например, среднеквадратичную)\n",
        "        output_layer = self.layers[-1]\n",
        "        mse = sum((expected_output[i] - output_layer.neurons[i].value)**2 for i in range(len(expected_output))) / len(expected_output)\n",
        "        return mse\n",
        "\n",
        "\n",
        "    # Обучение на всем наборе данных в течение нескольких эпох (итераций)\n",
        "    def train(self, dataset, epochs=1000):\n",
        "        print(f'\\nОБУЧЕНИЕ НАЧАТО ({epochs} эпох)...')\n",
        "        for epoch in range(epochs):\n",
        "            total_epoch_error = 0\n",
        "            # Перемешиваем датасет в начале каждой эпохи (для стохастичности)\n",
        "            random.shuffle(dataset)\n",
        "            for case in dataset:\n",
        "                input_data = case[0]\n",
        "                # Ожидаемый результат должен быть списком/кортежем\n",
        "                expected_res = case[1] if isinstance(case[1], (list, tuple)) else [case[1]]\n",
        "                case_error = self.train_once(input_data, expected_res)\n",
        "                total_epoch_error += case_error\n",
        "\n",
        "            # Выводим среднюю ошибку за эпоху (каждые N эпох для краткости)\n",
        "            if (epoch + 1) % 100 == 0 or epoch == 0:\n",
        "                avg_error = total_epoch_error / len(dataset)\n",
        "                print(f'Эпоха {epoch+1}/{epochs}, Средняя ошибка (MSE): {avg_error:.6f}')\n",
        "\n",
        "        print(f'\\nОБУЧЕНИЕ ЗАВЕРШЕНО!\\n')\n",
        "\n",
        "    # Тестирование сети на новых данных\n",
        "    def test(self, data, op_name=\"OPERATION\"):\n",
        "        print('\\nТЕСТИРОВАНИЕ ДАННЫХ:')\n",
        "        for case in data:\n",
        "            input_values = case # Входные данные для теста\n",
        "            self.set_input_data(input_values)\n",
        "            prediction = self.get_prediction() # Получаем предсказание\n",
        "            # Округляем результат для задач классификации (как OR)\n",
        "            rounded_res = [round(p) for p in prediction]\n",
        "            # Форматируем вывод\n",
        "            input_str = \", \".join(map(str, input_values))\n",
        "            pred_str = \", \".join(map(lambda x: f\"{x:.4f}\", prediction))\n",
        "            rounded_str = \", \".join(map(str, rounded_res))\n",
        "            print(f'{input_str} -> Предсказано: [{pred_str}] (Округленно: [{rounded_str}])')\n",
        "\n",
        "    # --- Сохранение и Загрузка Модели ---\n",
        "\n",
        "    # Сохранение обученной модели в файл\n",
        "    def save_model(self, filename=\"neuro_network_model.pkl\"):\n",
        "        try:\n",
        "            with open(filename, 'wb') as f:\n",
        "                pickle.dump(self, f)\n",
        "            print(f\"Модель успешно сохранена в файл: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при сохранении модели: {e}\")\n",
        "\n",
        "    # Загрузка модели из файла (статический метод, т.к. вызывается до создания объекта)\n",
        "    @staticmethod\n",
        "    def load_model(filename=\"neuro_network_model.pkl\"):\n",
        "        try:\n",
        "            with open(filename, 'rb') as f:\n",
        "                model = pickle.load(f)\n",
        "            print(f\"Модель успешно загружена из файла: {filename}\")\n",
        "            return model\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Ошибка: Файл модели не найден: {filename}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке модели: {e}\")\n",
        "            return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подключение диска"
      ],
      "metadata": {
        "id": "L90_p49MHA3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/MyDrive/')  # Просто подключаем весь диск"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD4jSGz9DaoU",
        "outputId": "951b795e-e3e2-4e34-eb19-c8cdf2443d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка датсета"
      ],
      "metadata": {
        "id": "cTcy9JxPJdWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запуск обучения"
      ],
      "metadata": {
        "id": "25jEOKCCHJQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Создание сети (2 входа, 1 выход для операции OR)\n",
        "\n",
        "\n",
        "# Попробуем скорость обучения поменьше (0.1) и больше эпох\n",
        "nn = NeuroNetwork(input_l_size=2, output_l_size=1, hidden_layers_count=1, learning_rate=0.1)\n",
        "\n",
        "# 2. Подготовка обучающего набора данных для операции \"ИЛИ\" (OR)\n",
        "# Формат: [[входные_данные], ожидаемый_выход]\n",
        "dataset_or = [\n",
        "    [[0, 0], [0]],\n",
        "    [[0, 1], [1]],\n",
        "    [[1, 0], [1]],\n",
        "    [[1, 1], [1]]\n",
        "]\n",
        "\n",
        "# 3. Обучение сети\n",
        "nn.train(dataset_or, epochs=10000) # Увеличим количество эпох\n",
        "\n",
        "# 4. Тестирование сети\n",
        "test_data_or = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "nn.test(test_data_or, 'OR')\n",
        "\n",
        "# 5. Сохранение обученной модели\n",
        "model_filename = \"my_or_network.pkl\"\n",
        "nn.save_model(model_filename)\n",
        "\n",
        "# --- Пример загрузки и использования сохраненной модели ---\n",
        "print(\"\\n--- Загрузка и тест сохраненной модели ---\")\n",
        "loaded_nn = NeuroNetwork.load_model(model_filename)\n",
        "\n",
        "if loaded_nn:\n",
        "    # Тестируем загруженную модель\n",
        "    loaded_nn.test(test_data_or, 'OR (loaded)')\n",
        "\n",
        "    # Можно даже дообучить загруженную модель, если нужно\n",
        "    # print(\"\\nДообучение загруженной модели...\")\n",
        "    # loaded_nn.train(dataset_or, epochs=1000)\n",
        "    # loaded_nn.test(test_data_or, 'OR (retrained)')\n",
        "else:\n",
        "    print(\"Не удалось загрузить модель для теста.\")"
      ],
      "metadata": {
        "id": "vz-cUrsU0PIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}